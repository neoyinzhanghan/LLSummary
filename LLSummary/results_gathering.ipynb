{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATHER RESULTS FOLDERS\n",
    "You add in CSVs of your cohort files in this block and the directory you want to save the results, and it will pool the slide results folders of your intended save directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from LLSummary.utils import rsync_with_retries, sftp_with_retries\n",
    "from LLRunner.config import results_dir\n",
    "\n",
    "cohort_files = []\n",
    "save_dir = \"/media/hdd3/greg/test\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "metadata_dict = {\n",
    "    \"cohort_file\": [],\n",
    "    \"wsi_name\": [],\n",
    "    \"username\": [],\n",
    "    \"hostname\": [],\n",
    "    \"machine\": [],\n",
    "    \"remote_result_dir\": [],\n",
    "    \"Dx\": [],\n",
    "    \"sub_Dx\": [],\n",
    "    \"datetime_processed\": [],\n",
    "    \"note\": [],\n",
    "}\n",
    "\n",
    "for cohort_file in cohort_files:\n",
    "    print(f\"Processing {cohort_file}.\")\n",
    "\n",
    "    df = pd.read_csv(cohort_file)\n",
    "\n",
    "    # Iterate over rows using tqdm\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        username = row[\"username\"]\n",
    "        hostname = row[\"hostname\"]\n",
    "        remote_result_dir = row[\"remote_result_dir\"]\n",
    "        remote_result_dir = os.path.join(results_dir, remote_result_dir)\n",
    "\n",
    "        # Define the local directory to save the data\n",
    "        local_dir = os.path.join(save_dir, os.path.basename(remote_result_dir))\n",
    "\n",
    "        # remove the local directory if it already exists, which means we always overwrite the data\n",
    "        # NOTE ths behaviour is expected because the data pooling right now is for viewing purposes only\n",
    "        if os.path.exists(local_dir):\n",
    "            os.rmdir(local_dir)\n",
    "            os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Run the rsync command with retries and exponential backoff\n",
    "        sftp_with_retries(\n",
    "            username,\n",
    "            hostname,\n",
    "            remote_result_dir,\n",
    "            local_dir,\n",
    "        )\n",
    "\n",
    "        # Add metadata to the metadata_dict\n",
    "        metadata_dict[\"cohort_file\"].append(cohort_file)\n",
    "        metadata_dict[\"wsi_name\"].append(row[\"wsi_name\"])\n",
    "        metadata_dict[\"username\"].append(username)\n",
    "        metadata_dict[\"hostname\"].append(hostname)\n",
    "        metadata_dict[\"machine\"].append(row[\"machine\"])\n",
    "        metadata_dict[\"remote_result_dir\"].append(row[\"remote_result_dir\"])\n",
    "        metadata_dict[\"Dx\"].append(row[\"Dx\"])\n",
    "        metadata_dict[\"sub_Dx\"].append(row[\"sub_Dx\"])\n",
    "        metadata_dict[\"datetime_processed\"].append(row[\"datetime_processed\"])\n",
    "        metadata_dict[\"note\"].append(row[\"note\"])\n",
    "\n",
    "# Save the metadata_dict as a DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_dict)\n",
    "\n",
    "# Save the metadata_df as a CSV file\n",
    "metadata_df.to_csv(os.path.join(save_dir, \"metadata.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Cell Cartridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from LLSummary.utils import rsync_with_retries, scp_with_retries, ssh_open_file\n",
    "from LLRunner.config import results_dir\n",
    "\n",
    "cohort_files = []\n",
    "save_dir = \"/media/hdd3/greg/test\"\n",
    "\n",
    "num_cartridges = 10\n",
    "\n",
    "metadata_dicts = []\n",
    "cell_names = [\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"E1\",\n",
    "    \"E4\",\n",
    "    \"ER1\",\n",
    "    \"ER2\",\n",
    "    \"ER3\",\n",
    "    \"ER4\",\n",
    "    \"ER5\",\n",
    "    \"ER6\",\n",
    "    \"L2\",\n",
    "    \"L4\",\n",
    "    \"M1\",\n",
    "    \"M2\",\n",
    "    \"M3\",\n",
    "    \"M4\",\n",
    "    \"M5\",\n",
    "    \"M6\",\n",
    "    \"MO2\",\n",
    "    \"PL2\",\n",
    "    \"PL3\",\n",
    "    \"U1\",\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(num_cartridges):\n",
    "    # make a directory for the cartridge in the save_dir\n",
    "    cartridge_dir = os.path.join(save_dir, f\"cartridge_{i}\")\n",
    "\n",
    "    os.makedirs(cartridge_dir, exist_ok=True)\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"cell_id\": [],\n",
    "        \"wsi_name\": [],\n",
    "        \"username\": [],\n",
    "        \"hostname\": [],\n",
    "        \"machine\": [],\n",
    "        \"remote_result_dir\": [],\n",
    "        \"original_name\": [],\n",
    "        \"Dx\": [],\n",
    "        \"sub_Dx\": [],\n",
    "        \"confidence\": [],\n",
    "        \"note\": [],\n",
    "        \"datetime_processed\": [],\n",
    "        \"label\": [],\n",
    "        \"VoL\": [],\n",
    "    }\n",
    "\n",
    "    for cellname in cell_names:\n",
    "        metadata_dict[cellname] = []\n",
    "\n",
    "    metadata_dicts.append(metadata_dict)\n",
    "\n",
    "# make a directory for the cartridge in the save_dir\n",
    "cartridge_dir = os.path.join(save_dir, f\"cartridge_{i}\")\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "cell_id = 0\n",
    "\n",
    "for cohort_file in cohort_files:\n",
    "\n",
    "    # Load the cohort file\n",
    "    df = pd.read_csv(cohort_file)\n",
    "\n",
    "    # Iterate over rows using tqdm\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        remote_result_dir = row[\"remote_result_dir\"]\n",
    "        remote_result_dir = os.path.join(results_dir, remote_result_dir)\n",
    "        username, hostname = row[\"username\"], row[\"hostname\"]\n",
    "\n",
    "        # Define the remote file path\n",
    "        cells_info_file = os.path.join(remote_result_dir, \"cells\", \"cells_info.csv\")\n",
    "\n",
    "        # Read the remote csv file using ssh_open_file (Assuming ssh_open_file reads the file from the remote system)\n",
    "        with ssh_open_file(username, hostname, cells_info_file) as f:\n",
    "            cells_info_df = pd.read_csv(f)\n",
    "\n",
    "        # randomly sample num_cartridges number of cells with replacement\n",
    "        sampled_cells_info_df = cells_info_df.sample(n=num_cartridges, replace=True)\n",
    "\n",
    "        # iterate over the sampled cells\n",
    "        for i in range(num_cartridges):\n",
    "            # get the row of the sampled cell df as dict\n",
    "            cell_info = sampled_cells_info_df.iloc[i].to_dict()\n",
    "\n",
    "            # get the name of the cell\n",
    "            name = cell_info[\"name\"]\n",
    "            label = cell_info[\"label\"]\n",
    "\n",
    "            # cell_path is remote_result_dir/cells/label/name\n",
    "            cell_path = os.path.join(remote_result_dir, \"cells\", label, name)\n",
    "\n",
    "            # define the local directory to save the data\n",
    "            cartridge_dir = os.path.join(save_dir, f\"cartridge_{i}\")\n",
    "            cell_dir = os.path.join(cartridge_dir, label)\n",
    "            cell_save_path = os.path.join(cell_dir, f\"{cell_id}.jpg\")\n",
    "\n",
    "            # scp the cell_path to the cell_save_path\n",
    "            scp_with_retries(username, hostname, cell_path, cell_save_path)\n",
    "\n",
    "            # add metadata to the metadata_dict\n",
    "            metadata_dicts[i][\"cell_id\"].append(cell_id)\n",
    "            metadata_dicts[i][\"wsi_name\"].append(row[\"wsi_name\"])\n",
    "            metadata_dicts[i][\"username\"].append(username)\n",
    "            metadata_dicts[i][\"hostname\"].append(hostname)\n",
    "            metadata_dicts[i][\"machine\"].append(row[\"machine\"])\n",
    "            metadata_dicts[i][\"remote_result_dir\"].append(row[\"remote_result_dir\"])\n",
    "            metadata_dicts[i][\"original_name\"].append(name)\n",
    "            metadata_dicts[i][\"Dx\"].append(row[\"Dx\"])\n",
    "            metadata_dicts[i][\"sub_Dx\"].append(row[\"sub_Dx\"])\n",
    "            metadata_dicts[i][\"confidence\"].append(row[\"confidence\"])\n",
    "            metadata_dicts[i][\"note\"].append(row[\"note\"])\n",
    "            metadata_dicts[i][\"datetime_processed\"].append(row[\"datetime_processed\"])\n",
    "            metadata_dicts[i][\"label\"].append(label)\n",
    "            metadata_dicts[i][\"VoL\"].append(cell_info[\"VoL\"])\n",
    "\n",
    "            for cellname in cell_names:\n",
    "                metadata_dicts[i][cellname].append(cell_info[cellname])\n",
    "\n",
    "            cell_id += 1\n",
    "\n",
    "# save the metadata_dicts as a list of DataFrames\n",
    "metadata_dfs = [pd.DataFrame(metadata_dict) for metadata_dict in metadata_dicts]\n",
    "\n",
    "# save the metadata_dfs as CSV files\n",
    "for i, metadata_df in enumerate(metadata_dfs):\n",
    "    metadata_df.to_csv(\n",
    "        os.path.join(save_dir, f\"cartridge_{i}\", \"metadata.csv\"), index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
